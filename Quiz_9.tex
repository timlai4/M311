\documentclass[addpoints]{exam}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{hyperref}

\newcommand{\R}{\mathbf{R}}
\newcommand{\Z}{\mathbf{Z}}

\pagestyle{headandfoot}
\runningheadrule
\firstpageheader{Math M311}{Quiz 9}{November 12, 2019}
\runningheader{Math M311}
{Quiz 9, Page \thepage\ of \numpages}
{November 12, 2019}
\firstpagefooter{}{}{}
\runningfooter{}{}{}

\begin{document}
\printanswers
\begin{questions}
\question[30] Maximize $f(x,y) = x + y$ where $x,y$ must lie on the unit circle. 
\begin{solution}
See \url{https://en.wikipedia.org/wiki/Lagrange_multiplier#Example_1}

$f(1/\sqrt{2}, 1/\sqrt{2}) = \sqrt{2}$
\end{solution}

\question[30] Maximize $f(p_1,p_2) = -(p_1 \log p_1 + p_2  \log p_2)$ subject to $p_1 + p_2 = 1$.  \\
The function is called information entropy and the $p_i$ are supposed to be probabilities. Therefore, you are determining the probability distribution that gives maximal information. 

\begin{solution}
$p_1 = 1/2 = p_2$, see below. 
\end{solution}

\bonusquestion[5] Given an arbitrary but still finite collection of probabilities, $\{p_1, \dots, p_n \}$, which means  $\sum_{i = 1}^n p_i = 1$, maximize 
\[
f(p_1, \dots, p_n ) = -\sum_{i = 1}^n p_i \log p_i
\]
What is the name of this probability distribution? Can you interpret this result in terms of entropy?

\begin{solution}
\url{https://en.wikipedia.org/wiki/Lagrange_multiplier#Example_3:_Entropy}
\end{solution}
\end{questions}
\end{document}